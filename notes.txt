PRIORITY list

* INSTALL USE CASES : -> USB_KEY ; SYSTEM ; ...
HOW TO ALOT HDs ... (thinking)

  -> just use yaml for now, create a profiler, that can for now be edited manually, soon enough graphically, or automation.
  
  : UPDATE    () - REWRITE BOOT_UPDATE 
  : MIRRORS   () - INCORPORATE 'HOSTS', & METRICS YAML ... BUILD DAEMON PROCESS, SERVICE FILES.

  : INSTALL   () - MIGRATE TO PYTHON (3.11), WRITE GOOD BASH BLOCKS FOR PYTHON INVOCATION -- begin installer GUI

  : MPM       () - BEGIN WRITING IN PYTHON (3.11)
  : PROFILES  () - BEGIN WRITING IN PYTHON (3.11)

  : BASTION   () - BEGIN WRITING 'syntax' IN PYTHON (DAEMON) (3.11)
                    syntax loads before the network, and actively monitors connections as they come up and go down, applying and adapting the rule set as needed, TRIGGER SERVICE RELOADS, as needed.
                    needs to work with NetworkManager, needs to work with null netifrc. one of the reasons, is OPENRC service files are not as well written as they should be. So be able to identify shortfalls, then cover for them.
                    to incorporate tools like selinux, bpf, track file handles, etc...
  : DEPLOY    (GOOD)
  : BULKBUILD (GOOD)
  : SYNC      (GOOD)

-----------

custom attributes :: { check for attributes vs calculated, store in custom.xxx }
profile attributes :: { check the attributes, depending on profile profile.xxx (profile.use) }
common attributes :: { check against common package apps common.xxx
meta attributes :: { check against installed meta apps meta.xxx

------------

dont forget to add all relavent etc files to patchfiles.... need better way to instantiate repo for patchfiles and backup method for patchfiles which are both local and remote managed.






------------

meta - packages
meta - modules 
  zfs
  nvidia
  vbox
    constraints (kernel version)
    associated packages
    associated keys/masks/license

    .. or : 

    scan world file, scan for default configs (license/keys/...)
    find all applicable configs for all packages in world file ... [ take existing configs && compare to applicable configs of @world ]
    find diff of the two, and create a 'custom.license custom.use custom...

    ... perhaps

------
above all else - create an initialization vector for the portage repo space

./sync.sh --initialize=/repo/space/ ... dumps a default host.cfg



science and guru missing from build repos config ... need a way to inject repos
need a script for ipmi-dell functions 
wrapper for compress/decompression using multithreading
prelim investigation in to multithreading emerge in order to process packages much quicker on many-core systems. 
------

yaml for benchmarking devices on busses
yaml for networking, adapter + outreach

qemu/virtual machine yaml function - list all virtual machines + attributes used in conjunction with meta install and for managing libvirt instances, also good for tying network mounts and for linking to network resources  

------ 

>

improve the boot yaml script to incorporate more variables, find more ways to ascertain states 
create a :

  fs_yaml w/ btrfs, ram+tmpfs, and zfs support    --> asset class calibration/ or allocated vs non-allocated opportunistic 

  logical space yaml (all existant file systems, performance-classes)
  pci_dev yaml (system_map?) to include cpu enumeration and attributes across all devices
  kernel yaml
  usb++ yaml (extensible|peripheral device busses)
  network_yaml (query local networks/devices/states/firewalls/attached helpers|BPF)

  create a schema for provisioning a hardware host, based on a loose targeting scheme  
>

ie. no more string single use files for modifying configs
yaml-conf for meta packages or system deployments = 

conf_root:
    portage:
      package:use
        default:
          app-editors:nano 
            set:
              -debug:
              -justify:
            unset:
              -magic:
              -minimal:

      package:keywords


      package:license

    modules:
      -module:
        -option

==? every list item is worked against the config file, issued by the list item under package:XXX

standardized files for portage:

package:
  license:
    common:
    build:
    meta_pkg_...
  unmask:
    ...
  mask:
    ...
  accept_keywords:
    ...
  use: 
    ...
    meta_pkg_libvirt
    meta_pkg_gimp
    ...
  repos.conf: ## SET BY DEPLOYMENT
    ...
  binrepos.conf:  ## PLACE HOLDER, IE CAN BE INFECTED BY OTHER REPOS ?
    ...


--------------------

add looking-glass to kvm 


---------

  need to add nvidia kernel module & ....

  have bash function which addresses all applicable kernel modules (from yaml file)

  and determines latest possible kernel version.

  this module structure will be added to the files in newly updated portage package. structure



------------------------------------------------------------ ** COMBINE REPO-OVERLAY AND REPO-{RSYNC|GENTOO} in to a seamless service, 
update service providers, metrics, and overlay access, create a roll of trouble tickets for all invalid URLs. 

fatal: unable to access 'https://git.sr.ht/bratishkaerik/bratishkaerik-overlay/': The requested URL returned error: 403

fatal: repository 'http://www.calculate-linux.org/' not found

fatal: repository 'fidonet' does not exist

fatal: repository 'https://github.com/gentoo-haskell/' not found

fatal: unable to access 'https://git.zero-downtime.net/quark/': The requested URL returned error: 503

fatal: repository 'https://cgit.gentoo.org/proj/ruby/' not found

fatal: unable to access 'http://overlay.xelnor.net/': SSL: no alternative certificate subject name matches target host name 'overlay.xelnor.net'

--> need a way to scan URL's for correct repo;
--> need a way to handle 403/503 errors
--> need a way to validate URL's
--> need a gitlab/github handler
--> need a way to copy in to tmp, then only modify changed files (wget->rsync)

==> repos are either direct download from http/s or git references w/ git access

--------------------------------------------------------------

move package.* to folders, all custom settings will remain, but default will be patched by sync

maintainGIT (include.sh) ... use this for maintaining git repos, and spitting out errors



--------------------------------------------------------------

need to implement meta inserts for : ## BUILD PROFILE WILL NEED TO ADDRESS 17.X or build variant {alpha,beta,gamma}
	
  ./deploy.sh 
      work={file space}

      build:alpha/desktop/hyprland/systemd
      
      deploy

---------------------------------------------------

all overlays must be maintained through a meta file, and in the repository file.

----------------------------------------------------

* on installs, use hw-probe as an option to upload specs to linux-hardware.org, a link to the profile can be used alongside any other localhost-webserve stuff, like the meta package: prometheus
* address keyboard type (us-...)

--------------------------------------------------------


NEW NOTES ...

*** NEED A WAY TO REFERENCE QUERIES ACROSS FILES, AND THRU SYNTAX ON 'VALUE' STATEMENTS -- LINKING 'TABLES' in effect. 

example: 

repos.eselect
repositories:{host.cfg/server:pkgROOT/root}



-----------------------------------------------------------

install appears to work okay, however, i need to verify that I can install to singular partitions, and modify existing boot drives.
Also, it appears refind.d is missing on new boot drives ; and there was some missing transfers on to the boot partition during install. 

------------------------------------------------------------

meta package manager (individual system applications) ... logs potential updates, can prompt user
meta system manager (applications which exist extemporaneously, on the network, and see to instantiate and regulate themselves over a subset of machines)
	:hosts
		this
		that
		there
	:application
		app_
	:configs
		a_
		b_

example of a MSM application (database:world_geodetic) [ architect ]
	- this requires several machines, some will cache locally, some will be a backup, some will be the prime server, others can load balance.
	- dns entries
	- services/service_routines
	- state_models (apiX) ... away to query the machine's stats, say load, etc.. in order to schedule optimally.
	- host listing/authentication, reliability prioritization, how to propagate a network, to find hosts not easily found (dns entries, or higher order auth queries can be used to search per host, other missing hosts on the network)
		- once hosts are resolved, missing or found, the net can optimize itself, and define the routes
		* the first self-optimizing net mapping routine will be used for installing a whole network at one time.

install : to->USB (uuid, host_string), to->HOST (id, host_string), to->NETWORK (hosts/host_string(s))

query system, caching and 

templates for machines (yaml_installs)	needs a profiler for ...

	cpu:
		caps
	cpu:
		caps
	ram
	peripherals
		network
			sfp+ x2
			10gb x2
		storage
			bus A
				disk a
				disk b
			bus B
			bus -usb

(templates)
	metal_dell_r730xd.yml
	metal_dell_r930.yml
	metal_dell_r630.yml
(example)
	jupiter.hypokrites.net.yml









profile_net	-	profile connection to network of hosts, given a spec file (hosts & stats)
		need to lookup maths for all applicable stats, used by sync, or install, or ... to update/install over multiple hosts.

profile_host	-	profile hardware on host (given a template/empty file)
		a template will verify first, and error out if configured differently (busses/missing peripherals) ... the template can be missing parameters, and this will be filled in, and not error out the process

install		->to USB 	; ->to HOST	; ->to NET

deploy

update

sync
	mirror


thinking about DOM_0

  keyserver
  ldap server (users / hosts / domains)
  dns ( in conjunction with ldap )
  build_server (gentoo) -- pkg.X.TLD
  profile manager + user-backup (block level rsync, ~/.**?)

* locales will move to install only, except for gen-all languages. 

* the initramfs module, has to be protected, and g1, has a key which are not accessable to g3. 

audits can be performed w/ a custom build, and the usb key inserted. 

* have to find a way to sign the block differential (zfs) between updates.
update process
install -> g1
@ install : clone g1@recovery -> g2
g2@ safe -> g3 (runtime)
update g2
g2@ xxxxxx  (while g3 running)
g2@safe -> (safe)
g2@xxxxxx -> (current)
g3 is cloned at boot by initramfs, previously one is deleted, or snapshot. 
w/ this design it is imperative to limit g2 to runtime, and all large delta directories are managed periodically on another dataset.

<CUSTOM> dynamic builds = date format in hex
<META ONLY> ordered builds = prime meta descriptor, ie for plasma, use the version of that, gnome, hardened would be libc, or llvm. 

move users to ./config ???
or just run a auth service ...

change key at the end of the install doesn't work as it appears the mounts are un'd.


# ENCRYPT patches, rebase all patchfile scripts, upon key change. (local build) use git, to track patch | date-dep variations.

ssh keygeneration on install, per user

how to score repos : 

    rate downloaded = x1 (GB)/transfer  ... higher rates indicate more interesting and fulfilling repo transactions
    speed downloaded = x2 (MBPS) ... higher rates indicate higher availability
    reliability = x3 (uptime percentage) ... higher rates indicate higher availability
    frequency used = x4 (times / week) ... preferably less than 2 times. depends on network availability tho
        ... indicative of distribution density

X - bindmounts / local only

how to score network

    hosts ... cumulative speed downloaded

    hosts ... reliability, mean deviation

    hosts ... spread_use, std deviation ... how distributed is the network utilized, ie, very much a few and very less a alot ... or very much a lot, and very less a few...

    USE YAML FILE TO GAUGE AVAILABILITY, AND PERFORMANCE OF NETWORK

    only list under 'protocols' if ever there was the ability to measure the service performance, otherwise, it will not be listed, however, it can be 'sensed' using a probe.

    discovering hosts/services - probe.sh
    utilization/metrics/measurement - sync.sh & probe.sh


    HOST:X
        hostname:pkg.hypokrites.me
        protocols:
            rsync
            http
            ftp
        protocol:rsync
            rate:100MBps
            peak:550MBps
            low:1MBps
            uptime:99%
            distribution:+3
            distributed:0
            'greeks'
            cumulative:2304123456KB
            utilization:
        protocol:http
            ...
        protocol:ftp
            ...
    HOST:Y



how to score aggregate network effect

*requires multiple hosts to distribute all results.




  ZFS TESTING FOR 2 WEEKS, following above, and CODE REVIEW + BELOW CHECKS, as appropriate.

  network is getting killed at the end of the build .... (deployment) ... WIFI ONLY.

goals : 

  eselect profile is being affected by a soft links being changed in /etc/portage ... figure it out 
  last seen june 1st, 2023

  x
  x 

  convert install to accept premade yaml configs, prompt for missing info ... working
  
  x

  need to fix unmount , its breaking current runtime. << can occur with bad profile ?
  
  x
  
  x

  x

\ INTEGRATION PLAN -proposal-
\ MIRROR
\ MGET
\ YAML
\ OTHER_COMPONENTS...

\ home\resource\package\meta.yml
\ home\resource\profile\meta.yml
\ home\code_type\submodule\executable
\ home\code_type\submodule\executable
\ home\about.txt

example.
\ home\g2Assets\bash\mirror\get_binpkgs.sh # wrapper script
\ home\g2Assets\bash\mirror\mirror.sh      # master
\ home\g2Assets\mirrors\hosts              # master_host_file
\ home\g2Assets\mirrors\binpkg             # derivative
\ home\g2Assets\mirrors\releases           # derivative
\ home\g2Assets\test\func.xml              # xml version of Input+Output+Func_Call
\ home\g2Assets\test\func.yml              # yaml version of IOF
\ home\g2Assets\test\func.out              # last results log
\ home\yamlBasic\
\ home\multiGet\
\ home\g2MetaPackageManager\
\ home\g2Sync\
\ home\g2Deploy\
\ home\g2Common\                            # common codebase
\ home\g2Install\
\ home\g2Update\


  - notes about swap systems observations of late (13 feb)
    need a profile service to restore configs, despite having old and complete etc.
    binpkgs+portage repo was out of touch/reach, vsftpd and lighttpd not installed, bastion doesn't address uninstalled services
    sure, a dom0 would be nice, but the ability to save profiles and launch from a usb-stick + repo w/ profiles is best.
    firefox could not emerge, possibly missed build deps. Should be a meta install anyways.
    meta packages need to be accomplished
    ZFS !!!! zfs does not import or mount -a -f. AF.
    

  - it appears that mesa + llvm are too old to support the XTX, so this would be considered a component patch
    meta file contains hardware spec, patch searches meta files, finds this condition, issues a patch, from a patch
	database / log, commands get executed, in this case it would be a mask on llvm and mesa. 0



  [scope]move build kernel to sync, install and update will pull in source/kernel... initramfs per kernel change or install. 

  migrate ssh-fuse links to nfsv4. Requires Dom0. 
  x 
  x
  need a way to guarantee awk '{print $x}' is where it should be. also sort by column guaranteed. [zsh and bash don't output the same!]
  x
  [waiting to reproduce] clear mounts is not nullifying tmp or other mounts unless in parent directory ,,.. verify/reproduce
  x
  need a tmpfs option for deployments, prior to copying. (--test-deployment) perhaps. work=/tmp/directory
  zfs functions appear broken, key change is complaining about leading slash in directory w/ given pool/dataset ?
  focus on installer, switch to yaml only configs (preconfig)
  create a tmpfs testing mounting scheme for installs --test
  integrate new kernels in update (--kernel)
  bring in a yaml<>refind filter, make adjustments in yaml, read to, write back.
  integrate a public-wireguard interface for all instances, same pre-shared key, inter-groper
    establish a 'protocol-lite' for inquiring about a local-net, works on SUB net as well. 
    INTER-NET will have to be prescribed, config'd, or db-lookup (DNS). 
    i.<domain>.tld = 10.0.0.0/8
    

    -- PER DOMAIN
    LOCAL-NET = 172.16-31.0.0/12  (local lan infrastructure)    HW.SWITCH   lan-router-dhcp
    MAC-NET = 192.168.0.0/16 (container space per VM, machine)  VIRBR0      fw-dhcp
    INTER-NET = 10.0.0.0/8 (tunnelling interface ip space)                  dom-o dhcp
    
    fw.branch-execution
                                                            0.0.0.0
    ------------------------------------------------> WAN             
      |           |                   |
      droplets    linode instance     cable_modem           (10.x.x.x)  24 bits
                                        |router             10.1.0.1  
                                        |
    ----------------------------------->| LAN               
        |       |       |   |   |       |machine            172.16.0.100
                            |                               (172.16-31.x.x) 20 bits
    ----------------------->| VIRBR0                        192.168.0.1
    |   |   |   |
    containers  web browser                                 (192.168.x.x)   16 bits

    IPV6 requires = 60 bits of addressable space for translating this scheme. 

    RFC 4193 Block 	Prefix/L 	Global ID (random) 	Subnet ID 	Number of addresses in subnet
	                  48 bits 	                    16 bits 	  64 bits
    fd00::/8 	      fd 	      xx:xxxx:xxxx 	      yyyy 	      18446744073709551616

    2^16 subnets ; [ 2^24 domains ; 2^20 machines ; 2^16 service providers ; 2^4 personalities ? ]



    veth -> virbr0 <bridge> ethX -> router -> wan_if <- router (keyhole)
    
    * every machine has a valid IP, with in the domain of local lan infrastructure (172)
    * every container has a wg instance, in the context of the subnet (192)
    * every router has a valid public IP, along with a wg (10) hub vif, 

    <bridge> real_ip (172,no masking) granted by router
    ^
    |
    |--------- default route 
    |
    |--------- container
    |
    |
    |VIRBR0




  (physical)



domain.tld
cloud_router (hub for domain / dom.0 service pointer)
10.0.0.1
load balancers can form bi,tri,quad,n shapes, the function of rerouting, dns regulation, would fall under the fw
example, a quad balancer, could serve 4 keyholes to different load functions/geographies. those geographies would 
have to start their router, say geoip or hostname would serve as a determinant, with respect to figuring /
prioritizing the balancer link, this would require a protocol to serve the arch. the fw could also share block-lists,
or other connection information. This protocol needs some attendance, it needs to be 'survivable' and self-aware.
intrinsic *geometric-balancing w/ prescribable operations/declarations. BPF. 

in the intermediary, a simple bastion.sh fw will serve as a basic fw, service provider, and basic logical subset for linking
across hardware/software. 


modem/router (out of bounds infrastructure)



  router  << responsible for routing to internet, transparently through unassociated networking (wg link)
  10.0.0.100

    w/s - serves as a router, for it's dependencies
    172.16.0.100
      vm1

      vm2

      container1
      container2
      container3

      default-route (local application route)

    server
    172.16.0.101
      vm1
      vm2
      vm3
      vm4
      vm5
      192.168.0.22

      containerX


    FIREFOX build dep issues
    
    dev-libs/boehm-gc -- missing dep dev-libs/libatomic_ops       // most likely a build script issue
    sys-libs/gpm -- /usr/bin/bison -- exec error format           //

  BUILD KERNEL ACCORDING TO ARCH IN HOST.CFG


  hierarchy ( dns records are retrieved through a secure keyhole )

    <domain>.tld (public facing interface)

    i.domain.tld = private.ip (lookup via private/secure dns only) [ auto config wg tunnel ]
    keyhole.domain.tld  (public service or app such as tunnelling, web-app, etc...)
    machine.domain.tld  (a floating ip, which is forwardable locally, ex. home computer )
    VM.domain.tld ( a VM which resides on a local machine, is routed through a virtual switch )
    service/app.machine.domain.tld - example { proxy.keyhole2.hypokrites.net } [ where the keyhole could be a proxy itself, in the cloud ]

------------------------------------------------------------------------------

  initramfs thinking : 

    for all installs, because a 'new' dataset, partition, or what ever is being added, there is an entirely
    new key being generated for the instance, because of this a new initramfs is always needed, unless there is no key.
    for all updates, involving new kernels, a new initramfs is always required.
    for all key-rolls, a new initramfs is needed. --rotate 

    update:

      --rotate = roll in new zfs keys, where ever applicable. old keys are never deleted, and backups can be sent to profile subsystem
      --kernel = update the kernel and associated modules, boot entries
      --user = ALWAYS used, portage/userspace



  Friday -                 
    KERNEL BLD service      (update)
    zfs page file generator (installer) 
    nvme zpool disk aggregation + yaml insertKeyvalue

  saturday
    installer

  Monday
    update

  Tuesday
    profile 1/2
  Wednesday
    profile 2/2
  Thursday
    profile testing


  finish install script, modify yaml (disks) at appropriate point
  fix update so that I can update local system and modify boot to current. Also make sure it uses proxy, not wan

  create test-maps for functions, basis functions have -t for basis checking other functions (higher up the stack)
  maps are json, ie { 3, "args", "this/is/a/path", "this/is/output" }, json maps can be stored in xml, alongside the include file.
  xml-map files will utilize 'jq' + xml query tools for commandline.
  https://stackoverflow.com/questions/1955505/parsing-json-with-unix-tools
  need json<>yaml conversion function ... or jq has to work pretty well, in case, universal object methods 

<span style="color:red">(not yet implemented) 
  (concept-btrfs) ./deploy build=plasma work=btrfs_mount/subvol\
  (concept-ext4,...) ./deploy build=plasma work=/path/to/rootfs deploy </span>

  the server still has issues running http+ services underload, causing missed mounts, missed downloads, etc...

[ BACKEND SPEC. | BUILD SERVER ]  :: { to reference pkgmx.sh & profile.sh for profiling machines and maintaining bin_pkgs }
  ?       TIME MACHINE BASED REPO REFERENCE (requires metadata build, snapshots...) - GOAL to hit year 2000 /w distfile fetcher
  ?       CUSTOM STAGE 3 GENERATOR FOR TIME MACHINE BUILDS
  ?       PER USE FLAG, PER VERSION, PACKAGE BUILDER (USE SUBVOLs or DATASET) ... BTRFS PROBABLY BEST SUITED FOR TREEING OUT VARIABLE CASE BUILDS
  ?       BUG TRACKING (FROM GENTOO.ORG) & LOCALLY GENERATED BUG REPORTING+LOGGING FACILITIES
  ?       AUTOMATED WORK AROUNDS (FIND WAYS TO TEST FOR WORK AROUNDS, AUTOMATICALLY, SAVE GOOD CATCH AS A PATCH, AND A BUG, W/ CLASS TYPE respecting the PATCH FORMULATION) [pluggable]
  - web serves are too inconsistent and will require 'tests'/QC after syncs/pulls (hashs most likely)
  - NOW MIGRATING TO DYNAMIC HOSTS, etc/hosts will be patched, soon a respectful dhcp/subnet-friendly/dns solution will be required for the dom.0 but before this, i will need to build NEXUS up.
  
  
  * VERIFY MODULES ARE INSTALLED ON INSTALL.

  - ADD FILE DIFF/STATS TO KERNEL MGET MIGRATE OVER, I WANT TO SEE HOW MUCH DATA & HOW MANY FILES CHANGED, is it worth it to diff copy over, instead of erase and duplicate ?

  - boot should have a yaml config, organize boot entries through a yaml filter function, open up to multiple boot loader/schemes.

  - skipping modules missing program (networking)
  - adsl/pppoe
  - br2684ctl
  - atmsigd/clipe
  - netplugd
  - ifplugd
  - ipppd
  - iwoconfig
  - firewald
  - udhcpc/busybox
  - pump
  - dhclient

  - automated issue tracking / bug reporting.
  - need to validate *.pkgs/dryrun before building in :: deploy. Some packages might not exist and will break the deployment
  - update needs to utilize 'profiles' in order to preserve profile specific config data 
  x
  - I seriously need a way to test package/profile combos and check for useflags/masking issues.
  x

  x
  - add meta commands to f/w (ex. fw.meta + fw.sh = tables) the meta file maps out invalid packet specs, and host configs. That said, the current intent to recurse a form of networking through all layers of the network stack, should be attempted, and then modeled afterwards, w/ in the meta config. 
  - better granularity over profile versions, and then move common in to specific versions, then be able to understand the version when patching w/ specific sets. 
  x
  
  - create an autopatcher script for updating servers, and software patches (basically a portage + sys patch hooked to a server update or emerge --update)
  - centralize the hosting-config for pkg/bld services with new host scripts. I need to be able to turn towards public or private, and between private servers with in one edit.
  - sanitize mget - stream methods, need to check this one closely for most/close to all cases.
  x
  x

  - install script is attempting to delete partitions after wiping the partition map.

  x

  ON INSTALL, the boot disk is not mounted to the zfs rootfs, thus refind is not updated...

  x

  - figure out why fstab is not loading up the pool/swap [install]

  - figure out how to have non-leaf nodes in YAML w/ forward slashes '/'

  ?
  ?
  ?
  
  x
  x

  add boot resolutions to the [profile], to be patched after deploy-system-[update], alongside the rest of the profile packages/configs.

  x
  x
  x

  add wpa_supplicant, and auto spawn for wireless, given an adapter, spawn can be used in fw meta package

      wireless = always LAN
      wired = check ip range, LAN = private space ; WAN = public space
      wireguard takes in all LAN + lo + default route
      virbr0 goes to WAN / static routes point to WAN-NET
      wireguard goes to WAN IP, need to define routing exchange

  IF A PACKAGE FAILS TO BUILD DURING DEPLOY, IT MUST BE:
    LOGGED
    & REPORTED 

  x
  x  

  ** META PACKAGE, FW NOTES:

    systemd-resolved disabled
    wireguard routing/instantiation/dependency installs
    openrc/systemd installer (./install.sh)
    assets: bastionX.sh, as (/etc/init.d - service) || (systemd unit files + sbin)
    dependency chains for net-tools & ipcalc
    bastionX, accounts for dynamic virttual bridging, and 1 wg interface, domain specific, for the machine its on 

  * bring vscode key, in to meta package, take out of default for desktop profiles.
  !!!!! move lxd-prep in to lxd meta package. virbr0 will be for libvirt, autosensed by fw, virbr1 reserved for lxd, autosensed.
  !!!!! run a service check (base services) w/ update (command [services]) on command line ./update.sh
  !!!!! NEED TO MOVE SCRIPT BASE DIR to project dir, and then prefix with bash, then move esync over to ./config, include stays in ./bash, along with mget, mirror moves to ./config --refactor
  //unmount.exe for unmounting chroots...

    x

  Prospect - profile.sh : ./profile.sh profile=sub.domain.tld work={pool/set}.../ bootpart=/dev/efi_part
  profile.sh - takes in the important settings for a given environment, needs to be modular, including the boot env, and stores them in ./profile/TLD/DOMAIN/SUB
  granularity is only down to subdomain, as this is meant for machines only,  not apps/services inside a cluster like kubernetes,  docker or lxd. 
  TBD.

  domain tools : have a config file for the domain name/server ips, manage all references through a single command/config file.
  x...
  create build services, to serve as an alternative for distfiles complete sync, where only relavent packages will be installed in to distfiles over sshfs/bind mounts. Build service to feature, binpkg versioning, per kernel, per glibc, ... use the local zfs on VM to snapshot per versioning, client versioning to use this as a basis, for it's own.
  g3 clones from g2 snapshot_versioning, from g1@safe.
  better snapshot management, especially for cloning to profile build services
  BTRFS INTEGRATION !!!
  kernel upgrade service, from upgrade script. (local machine upgrade in to kernel tree)
  add zfs key management, before dom-0 ca, must be able to integrate upwards/scale in to dom.0/ca
  x 
  x - partially resolved

  x
  x
  find a way to hash patched server directories, log output, and report differences
  bundles : need meta packages for things like libvirt, to include patches for /etc ... bundles can be placed in /bundle/call/*.pkgs;*.patches/rootdir/...
  x ... live environments/isos are not supported by refind, directly, would require separate gpt partitions, memtest added.
  x
  x
  add support for inclusion of arm+ builds { binpkgs | releases | ... }
  !!!!!! Add bastion4 to rc-conf.d :: new code in to bastion, wait for adapters, and timeout. Perhaps run as a daemon.
  MODIFY BASTION TO ASSOCIATE VIRTUAL BRIDGES, AUTOMATING FIND WAN ADAPTERS or LAN (based on Private Network Space/ vs WAN, possibly use conf.d/net ...) Need 
  x
  !!!!!!! verify nproc works on [update] ... needs to to be addressed, per boot (rc-conf.d) ... install to drive in live env, update not on live image, but to new host, and associate with boot medium. Live env=boot disk.
  CIK ---> live rescue image [boot] ---> install to host disk, update w/ CIK in computer, CIK adds a reference to the host system, rinse and repeat and you have one key for many systems.
  ON A multi-host system, the basis pool can facilitate many different CIK installs, each install is a profile, with a domain name, this is stored on the associated domain 0 for a given domain.
  Any time a CIK boots a host or rescue image, the initramfs can be swapped out for one with new keys, a rescue volume can have new keys loaded to it (post boot env). This is handled completely 
  transparently via ssh. Installs require network connectivity, or a an install derived from the rescue disk itself. 32 GB can facilitate up to two desktop systems, (20GB) and a 8GB EFI partition.
  CIK's up to 250GB can handle many different profiles, and more functionality perhaps, as well as maybe a roaming user space (volume). Disks up to 2TB can serve as a pop in domain 0, or perhaps even 
  another domain component/application service. See Dom.N / Dom.0. Every Dom. is a Virtual Machine, routed through virtual networking, interlinked via encrypted trunks. Every Dom provides Container Application Support.

  check if modules deployed during install (only)

  x
  x
  x
  x
  x

  libvirt is not patched. .... PROFILE ASSERT
  !!!!!!     world file, will be over written by portage map, need to rethink ...
  no network profiling ....
  x
  x
  qemu missing : swtpm + usermod-utilities, ssh enabled thru profile ... profile+services capture ... profile+key mngmt/capture
  x
  network addressing / configuration can take place using auto negotiation, utilizing certs, vpn-tun/tap's and dhcp. vpn's require a general client certificate + a private key/auth mechanism
  get rid of @safe snapshot prior to deployment, deployment assumes initialization ...
  !!!!!!!!!!!!!!!!!!!! all ip address scheme - dom.zero 10.0.0.1 (dns resolve pt) .: universal naming convention for the distribution hub, prod.dom.zero and dev.dom.zero
                      all other resolve points will be, if neccessary captured by a patch_sys -> /etc/hosts, for which the rest of the system can elate across the network through name services.
  install will assume the originating dataset's key and mount points, also install does not have a schema build system, where as multiple disks and custom properties cannot be asserted conveniently. 
  Should I give deployment an option for remote destination ? 
  Work+Verify I can install to an existing ZFS (only) pool, this skips disk initialization, setup, but requires a new boot entry
  Needs new_host.sh script to adapt new hardware (make.conf + networking) ; script is run 
  Needs dynamic keylocation assertion @ end of deploy.sh ... eventually tie in to CA/PKI/OLDAP w/ custom prebuilt initramfs 
  x
  x
  x
  need to start using signed packages !
  sequencing snapshots, versioning and updates.
  ....................need to auto configure zfs-loop for swap_memory+autofs.
  x
  x
  pkg signing keys + kernel_source (signing_module_key) needs to reside on Dom0
  EFI_signing , from safe_image, ... custom_key + appropriated_HW + procedure + thumb drive it seems ...
  try to find a way to assign keys from live environment (automated) 
  consider FUSE-encrypted file systems for cloud or publicly facing appliances. per process/user
  conversion script for pkg to signed package format : GPKG
  patch for roaming profiles ... also zsh.history_db integration.
  -user roaming (sync -OLDAP, ... ?)
  -machine roaming (snapshot w/ customization) .. think crypto_ignition_key<>
  -need a suplimentary dataset || subvol || partition setup for things like home. ... supplimentary configs ? (probably best for after GUI)
  :: perhaps a SSO - CIK, w/ an initramfs which overlays the roaming machine over a default, then snaps from r-o to rw g3 dataset (single boot)
 - dom0 needs to run on UTC
pkg-mx :
  use cases: REBUILD_MISSING_PKGS   # for x in $(echo "${_repositories}")
    # do
    #     _repo="$(findKeyValue "${SCRIPT_DIR}/config/host.cfg" "server:pkgROOT/repository/${x}")"
    #     printf "%s\n" "${_repository}${x} @ ${_repo}"
    #     [[ ! "$(cd ${_repository}${x} 2>/dev/null;git remote get-url origin)" == ${_repo} ]] && {
    #         [[ -d ${_repository}${x} ]] && { rm ${_repository}${x} -R; };
    #         git -C "${_repository}" clone ${_repo} ${x};
    #     } || {
    #         git -C "${_repository}${x}" fetch --all;
    #         git -C "${_repository}${x}" pull;
    #     };
    # done
portage/

  *infrastructure\
  distfiles       # install data\
  snapshots       # portage tree snapshots, daily or every other daily\
  repos           # repo portage tree, THE most up todate sync\
  releases        # sys releases repo, install medium / stage3\
  binpkgs         # binpkg repo
  
  *g2d\
  patchfiles      # specific config files (etc/...)\
  packages        # package and conf files, per profile, such as hardened, selinux, gnome, gnome/systemd\
  profiles        # configs for real/virtual machines, host/domain name dependent\
  kernels         # repo for current and depricated kernels




working on:

  btrfs+xfs+ext4 integration [ install.sh ]
  UPDATE deploy script, modernize w/ mget() on some ops, like install_kernel

needs:
  update function
  review install function
  further updates per f/s and schema added
  network adapter mapping (yaml...?)

CONCEPT:

  DOM-0 / 

    plug in to any machine, auto associates, connects to cloud VPN, auto updates/syncs.
    requirement - decent machine which can host a DOM-0 VM. 
    SIZE ... needs to be at least 8TB


#
##  INSTALLER NOTES
#S23 Ultra
#   requires:
#
#     off-premises kernel repository (~1TB+)
#     off-premises user repository (NAS,remote mount,NFSv4?)
#     off-premises standardized-root directory 
#     off-premises patchfiles (<100MB)
#     off-premises binpkgs--backup
#     generator for gentoo git repos  
#
#     g2deploy repository
#
#       linkage script, that pulls in repos
#       
#       create baseline snapshot, replaced monthly; periodic backups must be done on File Servers.
